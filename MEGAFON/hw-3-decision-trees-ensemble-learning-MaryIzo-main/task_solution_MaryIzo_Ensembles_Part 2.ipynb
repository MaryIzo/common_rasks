{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Ensembles\n",
    "\n",
    "В задачах нужно корректно реализовать функции, чтобы проходили тесты. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jvls9GQxWK5O"
   },
   "source": [
    "## 1. Bootstrap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9POKe84XWK6A"
   },
   "source": [
    "### Алгоритм Bootstrap \n",
    "* Равномерно возьмем из выборки $N$ объектов **с возвращением**. То есть мы хотим сгенерировать псевдовыборку, в которой могут повторятся элементы из исходной выборки. \n",
    "\n",
    "* Обозначим новую выборку через $X_1$. Повторяя процедуру $B$ раз, сгенерируем $M$ подвыборок $X_1, \\dots, X_B$. \n",
    "\n",
    "* Посчитаем статистику T от каждой выборки $(T(X_1), \\ldots, T(X_n))$\n",
    "\n",
    "* Найдем итоговую статистику $T(X) = \\frac{1}{B}\\sum^{B}_{i}T(X_i)$\n",
    "\n",
    "На вход массив чисел $X$ и число бутстрепных выборок $B$. Необходимо реализовать свой бутстреп и найти матожидание и стандартную ошибку у бутстрепных выборок.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oNTDVikgWK6F"
   },
   "source": [
    "### TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_awC3d6CWK6I"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import sem # ищет SE среднего\n",
    "\n",
    "def get_stats(X: np.array, B:int)->tuple:\n",
    "    '''\n",
    "        .∧＿∧ \n",
    "        ( ･ω･｡)つ━☆・*。 \n",
    "        ⊂  ノ    ・゜+. \n",
    "        しーＪ   °。+ *´¨) \n",
    "                .· ´¸.·*´¨) \n",
    "                (¸.·´ (¸.·'* ☆  <YOUR CODE>\n",
    "    '''\n",
    "    N = len(X)\n",
    "    mean, SE = 0, 0\n",
    "    \n",
    "    for i in range(B):\n",
    "        X_k = np.random.choice(X,N)\n",
    "        m, s = X_k.mean(), sem(X_k)\n",
    "        mean +=m\n",
    "        SE+=s\n",
    "        \n",
    "    mean=mean/B\n",
    "    SE=SE/B\n",
    "\n",
    "    return mean, SE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "X = np.array([37,43,38,36,17,40,40,45,41,84])\n",
    "B = 100000\n",
    "\n",
    "mean, se = get_stats(X, B)\n",
    "\n",
    "assert np.abs(mean - 42.1) < 0.05\n",
    "assert np.abs(se - 4.56) < 0.03\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимо реализовать свой небольшой беггинг на деревьях заданной грубины\n",
    "\n",
    "* бустингом сделать несколько выборок $X_1, \\ldots, X_B$\n",
    "* обучить на этих выборках алгоритмы: $a_1(\\cdot), \\ldots, a_B(\\cdot)$\n",
    "\n",
    "Получить результат беггинга как:\n",
    "$$a(x) = \\frac{1}{B}\\sum_{b=1}^{B}a_b(x)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor as DTR\n",
    "import random\n",
    "\n",
    "def bagging(X_train, y_train, X_test, boot_count, depth):\n",
    "    estimators = np.array([DTR(max_depth=depth) for _ in range(boot_count)])\n",
    "    \n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:YOUR CODE HERE\n",
    "    \n",
    "    y_pred = np.array( estimators[0].fit(X_train, y_train).predict(X_test))  \n",
    "\n",
    "    for i in range(1, boot_count):\n",
    "        \n",
    "        n = len(y_train)\n",
    "        \n",
    "        ind = [random.randint(0, n-1) for x in range(0, n)]\n",
    "        \n",
    "        X = np.array([X_train[ind[0]]] )\n",
    "        y = np.array([y_train[ind[0]]] )\n",
    "        \n",
    "        for j in ind[1:]:\n",
    "            X = np.append(X, [X_train[j]], axis = 0)\n",
    "            y = np.append(y, y_train[j])\n",
    "        \n",
    "        y_pred+= np.array( estimators[i].fit(X, y).predict(X_test))\n",
    "    \n",
    "    y_pred = y_pred/boot_count\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.647 6.053]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.testing import assert_array_equal, assert_array_almost_equal, assert_equal, assert_almost_equal\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "######################################################\n",
    "\n",
    "X_train = np.array([[0, 0], [1, 1], [5, 5], [8, 8], [10, 10]])\n",
    "y_train = np.array([0, 1, 5, 8, 10])\n",
    "X_test  = np.array([[4, 4], [6, 6]])\n",
    "y_test  = np.array([4, 6])\n",
    "\n",
    "B = 1000\n",
    "\n",
    "y_pred = bagging(X_train, y_train, X_test, boot_count=B, depth=3)\n",
    "print(y_pred)\n",
    "assert_array_almost_equal(y_pred, np.array([4, 6]), decimal=0)\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.14051578947369\n",
      "17.647697368421053\n",
      "14.306911699155107\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "X, y = load_boston(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=123,\n",
    "                                                    shuffle=True)\n",
    "\n",
    "y_pred = bagging(X_train, y_train, X_test, boot_count=200, depth=10)\n",
    "\n",
    "y_dt_pred = DecisionTreeRegressor().fit(X_train, y_train).predict(X_test)\n",
    "y = RandomForestRegressor().fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "print(mean_squared_error(y, y_test))\n",
    "print(mean_squared_error(y_dt_pred, y_test))\n",
    "print(mean_squared_error(y_pred, y_test))\n",
    "assert mean_squared_error(y_pred, y_test) < 15 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. X-regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимо найти наилучшие параметры для XGBRegression, обучить модель и вернуть ее. Данные берутся из папки data.\n",
    "\n",
    "Сам гридсерч или нативное исследование необходимо делать вне функции обработки, чтобы не получить TL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost.sklearn import XGBRegressor\n",
    "\n",
    "def xreg(X_train: np.array, y_train:np.array) -> XGBRegressor:\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    model = XGBRegressor(objective ='reg:squarederror', n_estimators = 6, max_depth = 7, random_state=17).fit(X_train, y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  76 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done 376 tasks      | elapsed:   18.6s\n",
      "[Parallel(n_jobs=-1)]: Done 398 out of 405 | elapsed:   21.2s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 405 out of 405 | elapsed:   22.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=XGBRegressor(base_score=0.5, booster='gbtree',\n",
       "                                    colsample_bylevel=1, colsample_bynode=1,\n",
       "                                    colsample_bytree=1, gamma=0,\n",
       "                                    importance_type='gain', learning_rate=0.1,\n",
       "                                    max_delta_step=0, max_depth=3,\n",
       "                                    min_child_weight=1, missing=None,\n",
       "                                    n_estimators=100, n_jobs=1, nthread=None,\n",
       "                                    objective='reg:squarederror',\n",
       "                                    random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "                                    scale_pos_weight=1, seed=None, silent=None,\n",
       "                                    subsample=1, verbosity=1),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'max_depth': range(1, 10),\n",
       "                         'n_estimators': range(1, 10)},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=True)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "df = pd.read_csv('Financial Distress.csv') \n",
    "\n",
    "X = df.drop('Financial Distress', axis=1)\n",
    "y = df['Financial Distress']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=17)\n",
    "\n",
    "\n",
    "xgb1 = XGBRegressor(objective ='reg:squarederror')\n",
    "\n",
    "n_estimators = range(1, 10)\n",
    "max_depth = range(1, 10)\n",
    "\n",
    "parameters = {'max_depth': max_depth, 'n_estimators': n_estimators}\n",
    "\n",
    "xgb_grid = GridSearchCV(xgb1, parameters, cv = 5, n_jobs = -1, verbose=True)\n",
    "\n",
    "xgb_grid.fit(X_train, y_train, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22721506335199831\n",
      "{'max_depth': 7, 'n_estimators': 6}\n",
      "2.264570030358505\n",
      "Well Done\n"
     ]
    }
   ],
   "source": [
    "print(xgb_grid.best_score_)\n",
    "print(xgb_grid.best_params_)\n",
    "\n",
    "t1 = time.time()\n",
    "xgb_model = xreg(X_train, y_train)\n",
    "t2 = time.time()\n",
    "assert t2 - t1 < 10\n",
    "\n",
    "\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "print(MSE(y_pred, y_test))\n",
    "\n",
    "assert type(xgb_model) == xgboost.sklearn.XGBRegressor\n",
    "assert MSE(y_pred, y_test) < 3\n",
    "print('Well Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. CatFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите модель классификации катбуста на предложенных данных и верните обученную модель. \n",
    "\n",
    "Воспользуйтесь встроенной обработкой категориальных признаков. Не забудьте обработать Nan значения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost\n",
    "def catfeatures(df: pd.DataFrame):\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    categorical = [ 'Month', 'DayofMonth', 'DayOfWeek', 'UniqueCarrier', 'Origin', 'Dest']\n",
    "    \n",
    "    df_tr = df.copy()\n",
    "    df_tr['dep_delayed_15min'] = df_tr['dep_delayed_15min'].replace('N', 0)\n",
    "    df_tr['dep_delayed_15min'] = df_tr[['dep_delayed_15min']].replace('Y', 1)\n",
    "    \n",
    "    df_tr = df_tr.drop(['dep_delayed_15min'], axis = 1).copy()\n",
    "    \n",
    "    train_data = catboost.Pool(data = df_tr, label=train_label, cat_features = categorical)\n",
    "    \n",
    "    model = catboost.CatBoostClassifier(iterations = 15, depth = 7)\n",
    "    model.fit(train_data)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.5\n",
      "0:\tlearn: 0.5435703\ttotal: 7ms\tremaining: 98ms\n",
      "1:\tlearn: 0.4953870\ttotal: 12.8ms\tremaining: 82.9ms\n",
      "2:\tlearn: 0.4765525\ttotal: 17.9ms\tremaining: 71.5ms\n",
      "3:\tlearn: 0.4673303\ttotal: 25.3ms\tremaining: 69.6ms\n",
      "4:\tlearn: 0.4612785\ttotal: 30.6ms\tremaining: 61.2ms\n",
      "5:\tlearn: 0.4584288\ttotal: 35.7ms\tremaining: 53.6ms\n",
      "6:\tlearn: 0.4543716\ttotal: 41.2ms\tremaining: 47.1ms\n",
      "7:\tlearn: 0.4536558\ttotal: 43.6ms\tremaining: 38.2ms\n",
      "8:\tlearn: 0.4520514\ttotal: 49.3ms\tremaining: 32.8ms\n",
      "9:\tlearn: 0.4498994\ttotal: 54.3ms\tremaining: 27.2ms\n",
      "10:\tlearn: 0.4458257\ttotal: 60.2ms\tremaining: 21.9ms\n",
      "11:\tlearn: 0.4448928\ttotal: 64.9ms\tremaining: 16.2ms\n",
      "12:\tlearn: 0.4420538\ttotal: 70.6ms\tremaining: 10.9ms\n",
      "13:\tlearn: 0.4406757\ttotal: 75.5ms\tremaining: 5.39ms\n",
      "14:\tlearn: 0.4389093\ttotal: 81ms\tremaining: 0us\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "#%%time\n",
    "df = pd.read_csv('flight_delays_train.csv')\n",
    "\n",
    "\n",
    "df_train = df[:7000]\n",
    "\n",
    "t1 = time.time()\n",
    "model = catfeatures(df_train)\n",
    "t2 = time.time()\n",
    "\n",
    "assert t2 - t1 < 10\n",
    "assert type(model) == catboost.CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well Done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "    \n",
    "df_test = pd.read_csv('flight_catfeature_test.csv')\n",
    "df_test = df_test.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "X_test = df_test.drop('dep_delayed_15min',axis=1)\n",
    "y_test = df_test['dep_delayed_15min']\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print('Well Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть мы хотим бустить регрессию со стандартной функцией потерь $MSE$:\n",
    "\n",
    "$$\\mathcal{L}(a, x,y) = (a(x_i) - y_i)^2$$\n",
    "\n",
    "Необходимо найти через взятие производных:\n",
    "\n",
    "1. Константный вектор $[f_0]_{i=1}^{N}$\n",
    "$$f_0(x) = \\arg\\min_{ c\\in \\mathbb{R}} \\sum_{i=1}^n \\mathcal{L}(c, y_i)$$ \n",
    "\n",
    "2. Градиенты функции потерь\n",
    "$$g_{i}^{t} = -\\Big[\\frac{\\partial \\mathcal{L}(f_t, x_i, y_i)}{\\partial f_t(x_i)}\\Big]_{i=1}^N$$\n",
    "\n",
    "3. Коэффициенты при композиции \n",
    "$$\\alpha_{t + 1} = \\arg\\min_\\alpha \\sum_{i=1}^N \\mathcal{L}(f_{t}(x_i) + \\alpha b_{t+1}(x_i), y_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def init(y_i: np.array) -> float:\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:\n",
    "    # (c - y1)^2 + (c - y2)^2 +...+ (c - yn)^2 = f Возьмем производную и приравняем к нулю, тогда c - это среднее от y.\n",
    "    f_0 = y_i.mean()\n",
    "    return f_0\n",
    "\n",
    "def grad(a: np.array, y: np.array) -> np.array:\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    # Тут по формуле для L расписываем\n",
    "    g = -2*(a-y)\n",
    "    return g\n",
    "\n",
    "def alpha(f :np.array, b: np.array, y: np.array) -> float:\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    alpha = ( (y*b).sum() - (f*b).sum())/( (b*b).sum())\n",
    "    # print(alpha)\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([1, 2, 3])\n",
    "f = np.array([2, 2, 2])\n",
    "b = np.array([0, 2, 4])\n",
    "\n",
    "f_0 = init(y)\n",
    "g = grad(f,y)\n",
    "al = alpha(f,b,y)\n",
    "\n",
    "\n",
    "assert np.abs(f_0 - 2.0)   < 1e-9\n",
    "assert_array_almost_equal(g, np.array([-2, 0, 2]))\n",
    "assert np.abs(al - (0.2)) < 1e-9\n",
    "# Мне кажется здесь alpha должен быть все-таки без минуса (если посчитать производную, там нет минуса)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well Done!\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "y = np.arange(20)\n",
    "f = np.ones(20) * 10\n",
    "b = np.arange(20) - 1\n",
    "\n",
    "f_0 = init(y)\n",
    "g = grad(f,y)\n",
    "al = alpha(f,b,y)\n",
    "\n",
    "\n",
    "assert np.abs(f_0 - 9.5)   < 1e-2\n",
    "assert_array_almost_equal(g, np.arange(-20,20, 2))\n",
    "assert np.abs(al - (0.2748)) < 1e-2\n",
    "print('Well Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. GradientBoosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте градиентный бустинг на решающих деревьях для регрессии с логгированием.Верните модель, которая будет хранить в себе `n_estimatos` обученных деревьев и коэффициенты, чтобы с их помощью потом найти результат предсказания.\n",
    "\n",
    "Также необходимо реализовать логгирование в течение обучения.\n",
    "* `self.estimators` - лист c деревьями\n",
    "* `self.alpha` - лист с коэффициентами альфа\n",
    "* `self.f_list` - лист со значениями комбинаций алгоритма $f_T(x_i) = f_0(x_i) + \\sum_{t=1}^{T}\\alpha_tb_t(x_i)$\n",
    "* `self.g_list` - лист с векторами градиентов на каждой итерации $g_{i}^{t} = -\\Big[\\frac{\\partial \\mathcal{L}(f_t, x_i, y_i)}{\\partial f_t(x_i)}\\Big]_{i=1}^N$\n",
    "* `self.b_list` - лист со значениями базового обучаемого дерева на тренировачной выборке на каждой итерации \n",
    "\n",
    "Примечания:\n",
    "* Обрывать алгоритм не нужно, необходимо обучить все деревья.\n",
    "* Начальный константный вектор из $f_0$ логгировать не нужно, однако не забудьте его добавить в `predict` c нужным количеством объектов!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor as DTR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class MyGradBoost():\n",
    "    def __init__(self, n_estimators=10, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.estimators_ = np.array([DTR(max_depth=self.max_depth) for _ in range(n_estimators)])\n",
    "        self.alpha = []\n",
    "        self.f_list = []\n",
    "        self.b_list = []\n",
    "        self.g_list = []\n",
    "        \n",
    "    def fit(self, X_train: np.array, y_train: np.array): \n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        \n",
    "        T = self.n_estimators\n",
    "        f_0 = init(y_train)\n",
    "        \n",
    "        self.f_list.append( [f_0]*len(y_train) )\n",
    "        self.g_list.append( grad( self.f_list[0], y_train) ) \n",
    "        self.b_list.append(1)\n",
    "        self.alpha.append(1)\n",
    "        \n",
    "        for t in range(1, T+1):\n",
    "            \n",
    "            self.estimators_[t-1] = self.estimators_[t-1].fit(X_train, self.g_list[t-1] )\n",
    "            self.b_list.append( self.estimators_[t-1].predict(X_train) )\n",
    "            self.alpha.append( alpha(self.f_list[t-1], self.b_list[t],  y_train  ) )  \n",
    "            self.f_list.append( self.f_list[t-1]+self.alpha[t]*self.b_list[t] )\n",
    "            self.g_list.append( grad(self.f_list[t], y_train) )\n",
    "          \n",
    "        return self\n",
    "        \n",
    "    def predict(self, X_test) -> np.array:\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        \n",
    "        T = self.n_estimators        \n",
    "        f_0 = self.f_list[0][0]\n",
    "        y_pred = np.array([f_0]*len(X_test))\n",
    "        for t in range(1, T+1):\n",
    "            y_pred += np.array( self.alpha[t]*self.estimators_[t-1].predict(X_test) ) \n",
    "        return y_pred\n",
    "    \n",
    "    def score(self, X_test, y_test)-> np.array:\n",
    "        return mean_squared_error(self.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06499999999999999\n"
     ]
    }
   ],
   "source": [
    "n_estimators = 2\n",
    "max_depth=3\n",
    "X_train = np.array([[0], [1], [2], [3], [4]])\n",
    "y_train = np.array([0, 2, 4, 2, 0])\n",
    "X_test  = np.array([[1.2], [2.3]])\n",
    "y_test  = np.array([2.2, 3.7])\n",
    "\n",
    "model = MyGradBoost(n_estimators=n_estimators, max_depth=max_depth).fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))\n",
    "assert model.score(X_test, y_test) < 0.2\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.014152544896247073\n",
      "Well Done\n"
     ]
    }
   ],
   "source": [
    "n_train, n_test, noise = 150, 1000, 0.1\n",
    "# Generate data\n",
    "def f(x):\n",
    "    x = x.ravel()\n",
    "    return np.exp(-x ** 2) + 1.5 * np.exp(-(x - 2) ** 2)\n",
    "\n",
    "def generate(n_samples, noise):\n",
    "    X = np.random.rand(n_samples) * 10 - 5\n",
    "    X = np.sort(X).ravel()\n",
    "    y = np.exp(-X ** 2) + 1.5 * np.exp(-(X - 2) ** 2)\\\n",
    "        + np.random.normal(0.0, noise, n_samples)\n",
    "    X = X.reshape((n_samples, 1))\n",
    "\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = generate(n_samples=n_train, noise=noise)\n",
    "X_test, y_test = generate(n_samples=n_test, noise=noise)\n",
    "\n",
    "\n",
    "model = MyGradBoost().fit(X_train, y_train)\n",
    "\n",
    "\n",
    "assert model.score(X_test, y_test) < 0.025\n",
    "\n",
    "\n",
    "model = MyGradBoost(n_estimators=100, \n",
    "                    max_depth=1).fit(X_train, y_train)\n",
    "\n",
    "print(model.score(X_test, y_test))\n",
    "assert model.score(X_test, y_test) < 0.017\n",
    "print('Well Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Самопроверка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def predict_and_plot(model, X_test, y_test, title):\n",
    "    y_predict = model.predict(X_test)\n",
    "\n",
    "    plt.plot(X_test, f(X_test), \"b\")\n",
    "    plt.scatter(X_train, y_train, c=\"b\", s=20)\n",
    "    plt.plot(X_test, y_predict, \"g\", lw=2)\n",
    "    plt.xlim([-5, 5])\n",
    "    plt.title(\"{} Loss: {:2f}\".format(title, model.score(X_test, y_test)))\n",
    "    plt.grid()\n",
    "\n",
    "\n",
    "\n",
    "model = MyGradBoost(n_estimators=30, \n",
    "                    max_depth=1).fit(X_train, y_train)\n",
    "\n",
    "ind =  [1,3,5,10,15,30]\n",
    "\n",
    "# GradientBoostingRegressor\n",
    "plt.plot(X_test, f(X_test), \"b\")\n",
    "plt.scatter(X_train, y_train, c=\"b\", s=20)\n",
    "n_est = [1,3,5,10,15,30]\n",
    "f = np.array(model.f_list)\n",
    "for i, n in enumerate(n_est):\n",
    "    colors = ['g', 'r', 'c', 'm', 'y', 'k']\n",
    "    plt.plot(X_train, f[n-1], color=colors[i], label=\"tree count={}\".format(n))\n",
    "\n",
    "plt.xlim([-5, 5])   \n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
